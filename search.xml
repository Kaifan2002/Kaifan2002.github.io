<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>SAM3部署跑通踩坑记录——用低版本CUDA118跑通SAM3</title>
      <link href="/2025/12/30/sam3install/"/>
      <url>/2025/12/30/sam3install/</url>
      
        <content type="html"><![CDATA[<p>感谢大佬Coding茶水间的博客<a href="https://volcengine.csdn.net/694cf32b5b9f5f31781aa064.html">SAM3模型来了，手把手带你运行SAM3模型代码，SAM3模型初探！</a>的指导<br>SAM3是2025年新发布的大模型，对配置要求较高，很多教程都提到了CUDA ≥ 12.6，但本人用组内的3090服务器（CUDA118）亲测可用，以下是本人的部署过程（踩坑指南），本人比较愚笨，只会用笨办法。<br>以下教程适合Linux，Win系统指路Coding茶水间大佬的文章。</p><h1>1、SAM3源码与权重下载</h1><p>SAM3的源码可在<a href="https://github.com/facebookresearch/sam3">gitbub</a>上下载，权重需要在Hugface上单独申请，但据说不太容易申请到，国内Modelscope上已经有大佬上传了权重，权重直接放在sam3路径下就可以。导入权重后需要按照第3章修改下配置。</p><h1>2、环境搭建</h1><p>环境搭建基本照抄官方教程<br><strong>创建新的 Conda 环境：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create -n sam3 python=3.12</span><br><span class="line">conda deactivate</span><br><span class="line">conda activate sam3</span><br></pre></td></tr></table></figure><p><strong>安装支持 CUDA 的 PyTorch：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#这里需要结合您自身的cuda版本，我这里是3090显卡，cuda版本118，因此和官方教程上的不一样</span><br><span class="line">pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  </span><br><span class="line">#建议提前搞好镜像源，不然会很慢</span><br></pre></td></tr></table></figure><p><strong>克隆仓库并安装包：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/facebookresearch/sam3.git #这步建议直接在github上下载，直接git有点慢</span><br><span class="line">cd sam3 # 如果是直接在git上下载解压的，那么路径就是sam3-main</span><br><span class="line">pip install -e . # 这步一般没什么问题</span><br></pre></td></tr></table></figure><h1>3、跑个实例</h1><p><strong>修改配置</strong><br>打开文件：sam3/model_builder.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">load_from_hf = <span class="literal">True</span> → 改成 <span class="literal">False</span></span><br><span class="line">checkpoint_path = <span class="literal">None</span> → 改成 <span class="string">&quot;sam3.pt&quot;</span></span><br></pre></td></tr></table></figure><p><strong>跑个实例</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里就是官网的案例，按自己的路径修改就可以，权重文件会自己读取</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#################################### For Image ####################################</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> sam3.model_builder <span class="keyword">import</span> build_sam3_image_model</span><br><span class="line"><span class="keyword">from</span> sam3.model.sam3_image_processor <span class="keyword">import</span> Sam3Processor</span><br><span class="line"><span class="comment"># Load the model</span></span><br><span class="line">model = build_sam3_image_model()</span><br><span class="line">processor = Sam3Processor(model)</span><br><span class="line"><span class="comment"># Load an image</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&quot;&lt;YOUR_IMAGE_PATH.jpg&gt;&quot;</span>)</span><br><span class="line">inference_state = processor.set_image(image)</span><br><span class="line"><span class="comment"># Prompt the model with text</span></span><br><span class="line">output = processor.set_text_prompt(state=inference_state, prompt=<span class="string">&quot;&lt;YOUR_TEXT_PROMPT&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the masks, bounding boxes, and scores</span></span><br><span class="line">masks, boxes, scores = output[<span class="string">&quot;masks&quot;</span>], output[<span class="string">&quot;boxes&quot;</span>], output[<span class="string">&quot;scores&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#################################### For Video ####################################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sam3.model_builder <span class="keyword">import</span> build_sam3_video_predictor</span><br><span class="line"></span><br><span class="line">video_predictor = build_sam3_video_predictor()</span><br><span class="line">video_path = <span class="string">&quot;&lt;YOUR_VIDEO_PATH&gt;&quot;</span> <span class="comment"># a JPEG folder or an MP4 video file</span></span><br><span class="line"><span class="comment"># Start a session</span></span><br><span class="line">response = video_predictor.handle_request(</span><br><span class="line">    request=<span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">type</span>=<span class="string">&quot;start_session&quot;</span>,</span><br><span class="line">        resource_path=video_path,</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line">response = video_predictor.handle_request(</span><br><span class="line">    request=<span class="built_in">dict</span>(</span><br><span class="line">        <span class="built_in">type</span>=<span class="string">&quot;add_prompt&quot;</span>,</span><br><span class="line">        session_id=response[<span class="string">&quot;session_id&quot;</span>],</span><br><span class="line">        frame_index=<span class="number">0</span>, <span class="comment"># Arbitrary frame index</span></span><br><span class="line">        text=<span class="string">&quot;&lt;YOUR_TEXT_PROMPT&gt;&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line">output = response[<span class="string">&quot;outputs&quot;</span>]</span><br></pre></td></tr></table></figure><h1>4、最坑的一集</h1><p>相信到这里您一定开跑了，然后报了一堆缺失库的错。我也很疑惑，我明明全程按照官方教程来的怎么还会报错，简直匪夷所思，而且前面安装过程也一直没出错，欢迎各位大佬指出我的漏洞。<br>接下来按照错误把缺失的包一个一个安装好就可以了，大概有五六七八个包。<br><img src="/images/OIP.jpg" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> 大模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>低光增强数据集</title>
      <link href="/2025/12/28/lowlight-dataset/"/>
      <url>/2025/12/28/lowlight-dataset/</url>
      
        <content type="html"><![CDATA[<p>本文汇总了目前主流的配对的低光图像数据集。</p><h1>1 LOLV数据集</h1><p>LOL数据集包含LOL-v1和LOL-v2两个版本，其中LOL-v2进一步划分为LOL-v2-real（真实世界数据）和LOL-v2-sync（合成数据）。具体而言，LOL-v1包含500对从真实自然场景中采集的低光/正常光图像，其中485对用于训练，15对用于测试。LOL-v2-real包含698对训练图像和100对测试图像；LOL-v2-sync则包括900对训练图像和100对测试图像。</p><h2 id="1-1-LOLv1数据集">1.1 LOLv1数据集</h2><p><a href="https://daooshee.github.io/BMVC2018website/">下载地址</a><br>来源文献：Deep Retinex Decomposition for Low-Light Enhancement<br><img src="/images/LOLV1NEW.png" alt="LOLV1"></p><h2 id="1-2-LOLv2数据集">1.2 LOLv2数据集</h2><p><a href="https://drive.google.com/file/d/1dzuLCk9_gE2bFF222n3-7GVUlSVHpMYC/view">下载地址</a><br>来源文献：Sparse gradient regularized deep retinex network for robust low-light image enhancement</p><ul><li><strong>Training Set:</strong> 689 images</li><li><strong>Testing Set:</strong> 100 images<br><img src="/images/LOLV2.png" alt="LOLV2"></li></ul><h1>2 SID数据集</h1><p><a href="https://storage.googleapis.com/isl-datasets/SID/Sony.zip">下载地址</a>   <a href="https://github.com/cchen156/Learning-to-See-in-the-Dark">Github详情页</a> Retinexformer作者也为我们提供了转换后的下载地址<a href="https://github.com/caiyuanhao1998/Retinexformer?tab=readme-ov-file">转换后的下载地址</a><br>来源文献：Chen Chen, Qifeng Chen, Minh N Do, and Vladlen Koltun. Seeing motion in the dark. In ICCV, 2019<br>数据集包含 5094 张原始短曝光图像，每张图像都有相应的长曝光参考图像。请注意，多个短曝光图像可以对应于同一个长曝光参考图像。例如，我们收集了短曝光图像序列来评估连发去噪方法。序列中的每个图像都算作一个不同的低光图像，因为每个这样的图像都包含真实的成像伪影，并且对训练和测试很有用。SID 中不同的长时间曝光参考图像的数量为 424。该数据集包含室内和室外图像。户外图像通常是在夜间、月光或街道照明下拍摄的。户外场景中相机处的照度一般在0.2勒克斯到5勒克斯之间。室内图像甚至更暗。他们是在封闭的房间里被捕获的，房间里关闭了常规的灯光，并为此目的设置了微弱的间接照明。室内场景中摄像机的照度一般在0.03勒克斯到0.3勒克斯之间。输入图像的曝光设置为1/30秒和1/10秒之间。相应的参考（地面实况）图像是在 100 到 300 倍的曝光时间下捕获的：即 10 到 30 秒。由于参考图像的曝光时间必然很长，因此数据集中的所有场景都是静态的。表1总结了数据集。参考图像的一小部分如图 2 所示。每个条件下大约 20% 的图像被随机选择来形成测试集，另外 10% 被选择用于验证集。图像是使用两台相机拍摄的：索尼 α7S II 和富士 X-T2。这些相机有不同的传感器：索尼相机有一个全画幅拜耳传感器和图 2。 富士相机有一个 APS-CX-Trans 传感器。这支持对不同滤波器阵列生成的图像进行低光图像处理管道的评估。索尼图像的分辨率为 4240×2832，富士图像的分辨率为 6000×4000。索尼套装是使用两种不同的镜头收集的。<br>有 2697 对短/长曝光 RAW 图像。低光/常光RGB图像是通过使用SID 相同的相机内信号处理将RAW转换为RGB来获得的.2099 和 598 个图像对用于训练和测试。<br><img src="/images/SID.png" alt=""></p><h1>3 SMID数据集</h1><p><a href="https://github.com/caiyuanhao1998/Retinexformer?tab=readme-ov-file">下载地址</a><br>来源文献：Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to see in the dark. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3291–3300, 2018. 6<br>全称是 <strong>See-in-the-Dark for Smartphone Images Dataset</strong>，SMID数据集总共包含20,809个短曝光/长曝光RAW图像对。同样需要将RAW数据转换为sRGB域以进行实验。使用 15,763 对进行训练，其余对用于测试。</p><p><img src="/images/SMID.png" alt=""></p><h1>4 LOLBlur数据集</h1><p><a href="https://github.com/sczhou/LEDNet">下载地址</a><br>文献来源：LEDNet: Joint Low-light Enhancement and Deblurring in the Dark (ECCV 2022)<br>这是一个以以逼真的方式生成的合成数据集，原作者引入了一种合成管道，该管道可以联合模拟低光模糊退化，从而能够生成大规模数据集（LOL-Blur）。共使用了 170 个用于训练的视频和 30 个用于测试的视频，每个视频有 60 帧，总计 12,000 个配对数据。生成的数据集 LOL-Blur 包含 12,000 对低模糊/正常锐度对，用于训练和测试。<br><img src="/images/LOLBLUR.png" alt=""></p><h1>5 MIT-Adobe FiveK数据集</h1><p><a href="https://github.com/caiyuanhao1998/Retinexformer?tab=readme-ov-file">下载地址</a>   <a href="https://data.csail.mit.edu/graphics/fivek/">数据发布页</a><br>来源文献：Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fre ́do Durand. Learning photographic global tonal adjustment with a database of input/output image pairs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 97–104, 2011. 7<br>MIT-Adobe FiveK 数据集分为训练集和测试集，分别有 4500 和 500 对低光/常光图像对。这些图像由五名摄影师手动调整（标记为 A∼E）。在Retinexforemr中以专家C的调整图像为参考，采用sRGB输出模式。</p><h1>6 SDSD数据集</h1><p><a href="https://github.com/caiyuanhao1998/Retinexformer?tab=readme-ov-file">下载地址</a>  <a href="https://github.com/dvlab-research/SDSD">数据发布页</a><br>SDSD 包含室内和室外子集。在Retinexformer中分别使用 62：6 和 116：10 低光/常光视频对对 SDSD-室内和 SDSD-室外进行训练和测试。<br><img src="/images/SDSD.png" alt=""></p><h1>7 LSRW</h1><p><a href="https://github.com/JianghaiSCU/R2RNet#dataset">下载地址</a><br>来源文献：R2RNet: Low-light Image Enhancement via Real-low to Real-normal Network<br>包括来自 DSLM Nikon 相机和华为智能手机的图像。LSRW-Nikon数据集由3150个训练图像对和20个测试图像对组成。LSRW-华为数据集分别包含 2450 对图像和 30 对用于训练和验证的图像。<br><img src="/images/LSRW.png" alt=""></p><h1>8 NTIRE 2024低光增强比赛数据集</h1><p><a href="https://github.com/caiyuanhao1998/Retinexformer?tab=readme-ov-file">下载地址</a>   <a href="https://codalab.lisn.upsaclay.fr/competitions/17640#learn_the_details">比赛介绍</a><br>来源文献：NTIRE2024ChallengeonLowLightImageEnhancement:MethodsandResults<br>引入了丰富的比赛场景，涵盖各种照明条件，如昏暗环境、极度黑暗、非条件，如昏暗环境、极度黑暗、非均匀照明、背光和夜景等。照明、背光和夜景等各种照明条件。适用于白天和夜晚的室内和室外环境。图像分辨率高达 4K 或更高。具体来说数据集包括 230 个训练场景，以及35 个验证场景和 35 个测试场景。验证和测试的地面实况（GT）验证和测试的地面实况（GT）图像在整个挑战赛期间对参赛者保密。</p><h1>FiveK</h1><p><a href="https://github.com/caiyuanhao1998/Retinexformer?tab=readme-ov-file">下载地址</a><br>来源文献：Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fre ́do Durand. Learning photographic global tonal adjustment with a database of input/output image pairs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 97–104, 2011. 7</p>]]></content>
      
      
      
        <tags>
            
            <tag> 低光增强 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>低光增强损失函数</title>
      <link href="/2025/12/28/lowlight-loss/"/>
      <url>/2025/12/28/lowlight-loss/</url>
      
        <content type="html"><![CDATA[<p>本文记载了在Low Light Image Enhancement （LLIE） 任务中常见的损失函数，并提供了相应的代码。其中的一些损失函数在其他Low Level任务中也可以适用。</p><h1>L1损失</h1><p>L1损失广泛应用在图像的各个任务中，<strong>L1损失</strong>（也称 <strong>Mean Absolute Error, MAE / 平均绝对误差</strong>）是深度学习和统计回归中常见的一种损失函数。它通过计算预测值与真实值之间的 <strong>绝对差值</strong> 来衡量误差：</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.819ex;" xmlns="http://www.w3.org/2000/svg" width="21.65ex" height="6.74ex" role="img" focusable="false" viewBox="0 -1733 9569.3 2978.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="4C" d="M62 -22T47 -22T32 -11Q32 -1 56 24T83 55Q113 96 138 172T180 320T234 473T323 609Q364 649 419 677T531 705Q559 705 578 696T604 671T615 645T618 623V611Q618 582 615 571T598 548Q581 531 558 520T518 509Q503 509 503 520Q503 523 505 536T507 560Q507 590 494 610T452 630Q423 630 410 617Q367 578 333 492T271 301T233 170Q211 123 204 112L198 103L224 102Q281 102 369 79T509 52H523Q535 64 544 87T579 128Q616 152 641 152Q656 152 656 142Q656 101 588 40T433 -22Q381 -22 289 1T156 28L141 29L131 20Q111 0 87 -11Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(723,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(681,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1885.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2941.6,0)"><g data-mml-node="mn" transform="translate(414,676)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220,-686)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><rect width="1088" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(4436.3,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path></g><g data-mml-node="TeXAtom" transform="translate(148.2,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(408,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(6047,0)"><g data-mml-node="mo"><svg width="278" height="1120" y="-310" x="27.5" viewBox="0 -138.7 278 1120"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" transform="scale(1,1.682)"></path></svg></g><g data-mml-node="msub" transform="translate(333,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1372.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(2372.4,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(300.6,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(3189.3,0)"><svg width="278" height="1120" y="-310" x="27.5" viewBox="0 -138.7 278 1120"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" transform="scale(1,1.682)"></path></svg></g></g></g></g></svg></mjx-container></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">Loss = nn.L1Loss() </span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">L1_loss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.loss = nn.L1Loss()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, light_map, normal_img</span>):</span><br><span class="line">        loss = <span class="variable language_">self</span>.loss(light_map, normal_img)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1>L2损失</h1><p>与L1损失一样，L2损失也是很常见的一种损失函数，但在低光增强任务中的应用比较少。L2 损失通过计算预测值与真实值之间 <strong>差值的平方</strong> 来度量误差：</p><p>$$<br>\mathcal{L}<em>{L2} = \frac{1}{N} \sum</em>{i=1}^{N} \left( y_i - \hat{y}_i \right)^2<br>$$</p><p><strong>优点</strong>：</p><ul><li><strong>平滑收敛</strong>：平方会让小误差贡献更小，大误差贡献更大，梯度会逐渐减小，更利于优化。</li><li><strong>数学性质好</strong>：连续可导，优化器更容易收敛。<br><strong>缺点</strong>：</li><li><strong>对异常值敏感</strong>：如果样本中有 outlier，平方会放大其影响，模型容易偏向异常点。</li><li><strong>容易模糊</strong>（在图像任务中）：因为它倾向于最小化整体均方误差，导致预测结果趋向“平均值”，图像增强和生成中可能出现平滑/模糊。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">criterion = nn.MSELoss() </span><br></pre></td></tr></table></figure><h1>VGG19损失函数——感知损失</h1><p>感知损失（Perceptual Loss）是一种基于深度学习的图像风格迁移方法中常用的损失函数。与传统的均方误差损失函数（Mean Square Error，MSE）相比，感知损失更注重图像的感知质量，更符合人眼对图像质量的感受。<br>感知损失是通过预训练的神经网络来计算两张图片之间的差异。通常使用预训练的卷积神经网络（Convolutional Neural Network，CNN），这些网络已经在大规模的数据集上进行了训练，可以提取图像的高级特征。例如，VGG-19网络中的卷积层可以提取图像的纹理和结构信息，而网络的全连接层可以提取图像的语义信息。<br>感知损失的计算方式通常是将输入图像和目标图像分别通过预训练的神经网络，得到它们在网络中的特征表示。然后将这些特征表示作为损失函数的输入，计算它们之间的欧氏距离或曼哈顿距离。感知损失的目标是最小化输入图像和目标图像在特征空间的距离。<br>下面的代码实例化 了一个VGG19网络，并在VGGloss中调用该网络，分别输入增强后的图像和正常图像，输出各层调整计算L1/L2损失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VGG19</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, requires_grad=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        vgg_pretrained_features = torchvision.models.vgg19(weights=torchvision.models.VGG19_Weights.IMAGENET1K_V1).features</span><br><span class="line">        <span class="variable language_">self</span>.slice1 = torch.nn.Sequential()</span><br><span class="line">        <span class="variable language_">self</span>.slice2 = torch.nn.Sequential()</span><br><span class="line">        <span class="variable language_">self</span>.slice3 = torch.nn.Sequential()</span><br><span class="line">        <span class="variable language_">self</span>.slice4 = torch.nn.Sequential()</span><br><span class="line">        <span class="variable language_">self</span>.slice5 = torch.nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">            <span class="variable language_">self</span>.slice1.add_module(<span class="built_in">str</span>(x), vgg_pretrained_features[x])</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="number">7</span>):</span><br><span class="line">            <span class="variable language_">self</span>.slice2.add_module(<span class="built_in">str</span>(x), vgg_pretrained_features[x])</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">7</span>, <span class="number">12</span>):</span><br><span class="line">            <span class="variable language_">self</span>.slice3.add_module(<span class="built_in">str</span>(x), vgg_pretrained_features[x])</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">12</span>, <span class="number">21</span>):</span><br><span class="line">            <span class="variable language_">self</span>.slice4.add_module(<span class="built_in">str</span>(x), vgg_pretrained_features[x])</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">21</span>, <span class="number">30</span>):</span><br><span class="line">            <span class="variable language_">self</span>.slice5.add_module(<span class="built_in">str</span>(x), vgg_pretrained_features[x])</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> requires_grad:</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line">                param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        h_relu1 = <span class="variable language_">self</span>.slice1(X)</span><br><span class="line">        h_relu2 = <span class="variable language_">self</span>.slice2(h_relu1)</span><br><span class="line">        h_relu3 = <span class="variable language_">self</span>.slice3(h_relu2)</span><br><span class="line">        h_relu4 = <span class="variable language_">self</span>.slice4(h_relu3)</span><br><span class="line">        h_relu5 = <span class="variable language_">self</span>.slice5(h_relu4)</span><br><span class="line">        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VGGLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, loss_weight=<span class="number">1.0</span>, criterion = <span class="string">'l1'</span>, reduction=<span class="string">'mean'</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(VGGLoss, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.vgg = VGG19().cuda()</span><br><span class="line">        <span class="keyword">if</span> reduction <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'none'</span>, <span class="string">'mean'</span>, <span class="string">'sum'</span>]:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f'Unsupported reduction mode: <span class="subst">{reduction}</span>. '</span></span><br><span class="line">                             <span class="string">f'Supported ones are: <span class="subst">{_reduction_modes}</span>'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> criterion == <span class="string">'l1'</span>:</span><br><span class="line">            <span class="variable language_">self</span>.criterion = nn.L1Loss(reduction=reduction)</span><br><span class="line">        <span class="keyword">elif</span> criterion == <span class="string">'l2'</span>:</span><br><span class="line">            <span class="variable language_">self</span>.criterion = nn.MSELoss(reduction=reduction)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="string">'Unsupported criterion loss'</span>)</span><br><span class="line">  </span><br><span class="line">        <span class="variable language_">self</span>.weights = [<span class="number">1.0</span> / <span class="number">32</span>, <span class="number">1.0</span> / <span class="number">16</span>, <span class="number">1.0</span> / <span class="number">8</span>, <span class="number">1.0</span> / <span class="number">4</span>, <span class="number">1.0</span>]</span><br><span class="line">        <span class="variable language_">self</span>.weight = loss_weight</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        x_vgg, y_vgg = <span class="variable language_">self</span>.vgg(x), <span class="variable language_">self</span>.vgg(y)</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_vgg)):</span><br><span class="line">            loss += <span class="variable language_">self</span>.weights[i] * <span class="variable language_">self</span>.criterion(x_vgg[i], y_vgg[i].detach())</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.weight * loss</span><br></pre></td></tr></table></figure><p>另一种写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line"><span class="comment"># other import</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> exp</span><br><span class="line"><span class="keyword">import</span> pytorch_msssim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MeanShift</span>(nn.Conv2d):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rgb_range, rgb_mean, rgb_std, sign=-<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MeanShift, <span class="variable language_">self</span>).__init__(<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        std = torch.Tensor(rgb_std)</span><br><span class="line">        <span class="variable language_">self</span>.weight.data = torch.eye(<span class="number">3</span>).view(<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.weight.data.div_(std.view(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="variable language_">self</span>.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)</span><br><span class="line">        <span class="variable language_">self</span>.bias.data.div_(std)</span><br><span class="line">        <span class="variable language_">self</span>.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VGGLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, conv_index=<span class="string">'54'</span>, rgb_range=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(VGGLoss, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        vgg_features = models.vgg19(pretrained=<span class="literal">True</span>).features</span><br><span class="line">        modules = [m <span class="keyword">for</span> m <span class="keyword">in</span> vgg_features]</span><br><span class="line">        <span class="keyword">if</span> conv_index == <span class="string">'22'</span>:</span><br><span class="line">            <span class="variable language_">self</span>.vgg = nn.Sequential(*modules[:<span class="number">8</span>])</span><br><span class="line">            <span class="variable language_">self</span>.vgg.cuda()</span><br><span class="line">        <span class="keyword">elif</span> conv_index == <span class="string">'54'</span>:</span><br><span class="line">            <span class="variable language_">self</span>.vgg = nn.Sequential(*modules[:<span class="number">35</span>])</span><br><span class="line">            <span class="variable language_">self</span>.vgg.cuda()</span><br><span class="line"></span><br><span class="line">        vgg_mean = (<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>)</span><br><span class="line">        vgg_std = (<span class="number">0.229</span> * rgb_range, <span class="number">0.224</span> * rgb_range, <span class="number">0.225</span> * rgb_range)</span><br><span class="line">        <span class="variable language_">self</span>.sub_mean = MeanShift(rgb_range, vgg_mean, vgg_std).cuda()</span><br><span class="line">        <span class="variable language_">self</span>.vgg.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, sr, hr</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">x</span>):</span><br><span class="line">            x = <span class="variable language_">self</span>.sub_mean(x)</span><br><span class="line">            x = <span class="variable language_">self</span>.vgg(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line">      </span><br><span class="line">        vgg_sr = _forward(sr)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            vgg_hr = _forward(hr.detach())</span><br><span class="line"></span><br><span class="line">        loss = F.mse_loss(vgg_sr, vgg_hr)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>与一般的L1损失和L2损失对比，感知损失有以下特点：</p><table><thead><tr><th>特征</th><th>像素级损失(L1/L2)</th><th>感知损失</th></tr></thead><tbody><tr><td>比较空间</td><td>原始像素空间</td><td>深度特征空间</td></tr><tr><td>关注点</td><td>像素值精确匹配</td><td>语义内容匹配</td></tr><tr><td>结果</td><td>可能模糊/不自然</td><td>更自然、更符合视觉感知</td></tr><tr><td>计算复杂度</td><td>低</td><td>较高(需要前向传播)</td></tr></tbody></table><p>通常，感知损失是由L1损失和L2损失来计算的，表达式如下：</p><p>$$<br>\begin{equation}<br>\mathcal{L}<em>{\text{perceptual}}^{L1} = \sum</em>{l} \lambda_l \left| \phi_l(I_{\text{gen}}) - \phi_l(I_{\text{ref}}) \right|_1<br>\end{equation}<br>$$</p><p>$$<br>\begin{equation}<br>\mathcal{L}<em>{\text{perceptual}}^{L2} = \sum</em>{l} \lambda_l \left| \phi_l(I_{\text{gen}}) - \phi_l(I_{\text{ref}}) \right|_2^2<br>\end{equation}<br>$$</p><h1>边缘损失</h1><p>边缘损失常用于低光增强、超分辨、去噪等重建类任务里。它的目标是让预测图像和真实图像在 边缘结构上尽量接近，从而避免结果模糊。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EdgeLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(EdgeLoss, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.sobel_kernel_x = torch.tensor([[-<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>], [-<span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>], [-<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]], dtype=torch.float32).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="variable language_">self</span>.sobel_kernel_y = torch.tensor([[-<span class="number">1</span>, -<span class="number">2</span>, -<span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]], dtype=torch.float32).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, x_hat</span>):</span><br><span class="line">        <span class="variable language_">self</span>.sobel_kernel_x = <span class="variable language_">self</span>.sobel_kernel_x.to(x.device)</span><br><span class="line">        <span class="variable language_">self</span>.sobel_kernel_y = <span class="variable language_">self</span>.sobel_kernel_y.to(x.device)</span><br><span class="line">        grad_x = torch.zeros_like(x)</span><br><span class="line">        grad_x_hat = torch.zeros_like(x_hat)</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">1</span>]):</span><br><span class="line">            grad_x_c_x = F.conv2d(x[:, c:c+<span class="number">1</span>, :, :], <span class="variable language_">self</span>.sobel_kernel_x, padding=<span class="number">1</span>)</span><br><span class="line">            grad_x_c_y = F.conv2d(x[:, c:c+<span class="number">1</span>, :, :], <span class="variable language_">self</span>.sobel_kernel_y, padding=<span class="number">1</span>)</span><br><span class="line">            grad_x[:, c:c+<span class="number">1</span>, :, :] = torch.sqrt(grad_x_c_x ** <span class="number">2</span> + grad_x_c_y ** <span class="number">2</span> + <span class="number">1e-6</span>)</span><br><span class="line">            grad_x_hat_c_x = F.conv2d(x_hat[:, c:c+<span class="number">1</span>, :, :], <span class="variable language_">self</span>.sobel_kernel_x, padding=<span class="number">1</span>)</span><br><span class="line">            grad_x_hat_c_y = F.conv2d(x_hat[:, c:c+<span class="number">1</span>, :, :], <span class="variable language_">self</span>.sobel_kernel_y, padding=<span class="number">1</span>)</span><br><span class="line">            grad_x_hat[:, c:c+<span class="number">1</span>, :, :] = torch.sqrt(grad_x_hat_c_x ** <span class="number">2</span> + grad_x_hat_c_y ** <span class="number">2</span> + <span class="number">1e-6</span>)</span><br><span class="line">        loss = F.mse_loss(grad_x, grad_x_hat)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h1>SSIM损失</h1><p>SSIM指标介于0到1直接，且越大说明模型效果越好，而SSIM损失就是：</p><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="22.545ex" height="1.952ex" role="img" focusable="false" viewBox="0 -705 9964.9 862.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(645,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(1290,0)"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="msub" transform="translate(1794,0)"><g data-mml-node="mi"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1003,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(783,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(1252,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4341.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(5397.5,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(6119.7,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(7119.9,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(7764.9,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(8409.9,0)"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="mi" transform="translate(8913.9,0)"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g></g></g></svg></mjx-container></p><p>SSIM的原理与具体计算方法不在这里展开</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SSIM_loss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, window_size=<span class="number">11</span>, sigma=<span class="number">1.5</span>, data_range=<span class="number">1.0</span>, channel=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SSIM_loss, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.window_size = window_size</span><br><span class="line">        <span class="variable language_">self</span>.sigma = sigma</span><br><span class="line">        <span class="variable language_">self</span>.data_range = data_range</span><br><span class="line">        <span class="variable language_">self</span>.channel = channel</span><br><span class="line">        <span class="variable language_">self</span>.gaussian_kernel = <span class="variable language_">self</span>._create_gaussian_kernel(window_size, sigma)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_create_gaussian_kernel</span>(<span class="params">self, window_size, sigma</span>):</span><br><span class="line">        gauss = torch.Tensor([np.exp(-(x - window_size//<span class="number">2</span>)**<span class="number">2</span>/<span class="built_in">float</span>(<span class="number">2</span>*sigma**<span class="number">2</span>)) </span><br><span class="line">                            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(window_size)])</span><br><span class="line">        gauss = gauss / gauss.<span class="built_in">sum</span>()</span><br><span class="line">        kernel = torch.outer(gauss, gauss)</span><br><span class="line">        <span class="keyword">return</span> kernel.view(<span class="number">1</span>, <span class="number">1</span>, window_size, window_size).repeat(<span class="variable language_">self</span>.channel, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ssim</span>(<span class="params">self, img1, img2</span>):</span><br><span class="line">        C1 = (<span class="number">0.01</span> * <span class="variable language_">self</span>.data_range)**<span class="number">2</span></span><br><span class="line">        C2 = (<span class="number">0.03</span> * <span class="variable language_">self</span>.data_range)**<span class="number">2</span></span><br><span class="line">        kernel = <span class="variable language_">self</span>.gaussian_kernel.to(img1.device)</span><br><span class="line">        mu1 = F.conv2d(img1, kernel, padding=<span class="number">0</span>, groups=<span class="variable language_">self</span>.channel)</span><br><span class="line">        mu2 = F.conv2d(img2, kernel, padding=<span class="number">0</span>, groups=<span class="variable language_">self</span>.channel)</span><br><span class="line">        mu1_sq = mu1.<span class="built_in">pow</span>(<span class="number">2</span>)</span><br><span class="line">        mu2_sq = mu2.<span class="built_in">pow</span>(<span class="number">2</span>)</span><br><span class="line">        mu1_mu2 = mu1 * mu2</span><br><span class="line">        sigma1_sq = F.conv2d(img1 * img1, kernel, padding=<span class="number">0</span>, groups=<span class="variable language_">self</span>.channel) - mu1_sq</span><br><span class="line">        sigma2_sq = F.conv2d(img2 * img2, kernel, padding=<span class="number">0</span>, groups=<span class="variable language_">self</span>.channel) - mu2_sq</span><br><span class="line">        sigma12 = F.conv2d(img1 * img2, kernel, padding=<span class="number">0</span>, groups=<span class="variable language_">self</span>.channel) - mu1_mu2</span><br><span class="line">        ssim_map = ((<span class="number">2</span> * mu1_mu2 + C1) * (<span class="number">2</span> * sigma12 + C2)) / \</span><br><span class="line">                  ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))</span><br><span class="line">  </span><br><span class="line">        <span class="keyword">return</span> ssim_map.mean()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img1, img2</span>):</span><br><span class="line">        <span class="keyword">if</span> img1.size() != img2.size():</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f"Input images must have the same dimensions. Got <span class="subst">{img1.size()}</span> and <span class="subst">{img2.size()}</span>"</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.channel != img1.shape[<span class="number">1</span>]:</span><br><span class="line">            <span class="variable language_">self</span>.channel = img1.shape[<span class="number">1</span>]</span><br><span class="line">            <span class="variable language_">self</span>.gaussian_kernel = <span class="variable language_">self</span>._create_gaussian_kernel(<span class="variable language_">self</span>.window_size, <span class="variable language_">self</span>.sigma)</span><br><span class="line">        ssim_val = <span class="variable language_">self</span>.ssim(img1, img2)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> - ssim_val</span><br></pre></td></tr></table></figure><h1>频域损失</h1><p>随着频域处理引入低光增强领域，频域损失也在低光增强中得到应用，主要目的是让预测图与真值在 频率域的能量分布上保持一致</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FrequencyLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, loss_weight = <span class="number">0.01</span>, criterion =<span class="string">'l1'</span>, reduction = <span class="string">'mean'</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FrequencyLoss, <span class="variable language_">self</span>).__init__()   </span><br><span class="line">        <span class="variable language_">self</span>.loss_weight = loss_weight</span><br><span class="line">        <span class="variable language_">self</span>.reduction = reduction</span><br><span class="line">        <span class="keyword">if</span> criterion == <span class="string">'l1'</span>:</span><br><span class="line">            <span class="variable language_">self</span>.criterion = nn.L1Loss(reduction=reduction)</span><br><span class="line">        <span class="keyword">elif</span> criterion == <span class="string">'l2'</span>:</span><br><span class="line">            <span class="variable language_">self</span>.criterion = nn.MSELoss(reduction=reduction)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError(<span class="string">'Unsupported criterion loss'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pred, target, weight=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        pred_freq = <span class="variable language_">self</span>.get_fft_amplitude(pred)</span><br><span class="line">        target_freq = <span class="variable language_">self</span>.get_fft_amplitude(target)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.loss_weight * <span class="variable language_">self</span>.criterion(pred_freq, target_freq)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_fft_amplitude</span>(<span class="params">self, inp</span>):</span><br><span class="line">        inp_freq = torch.fft.rfft2(inp, norm=<span class="string">'backward'</span>)</span><br><span class="line">        amp = torch.<span class="built_in">abs</span>(inp_freq)</span><br><span class="line">        <span class="keyword">return</span> amp</span><br></pre></td></tr></table></figure><h1>Charbonnier Loss</h1><p>发表于《Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid Networks》<br>与L1相比，增加了一个正则项。用Charbonnier Loss来近似L1损失来提高模型的性能,接近零点的值的梯度由于ε的存在，梯度不会太小，避免梯度消失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">L1_Charbonnier_loss</span>(torch.nn.Module):</span><br><span class="line">    <span class="string">"""L1 Charbonnierloss."""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(L1_Charbonnier_loss, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.eps = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, Y</span>):</span><br><span class="line">        diff = torch.add(X, -Y)</span><br><span class="line">        error = torch.sqrt(diff * diff + <span class="variable language_">self</span>.eps)</span><br><span class="line">        loss = torch.mean(error)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h1>颜色一致性损失</h1><p>这里提到的颜色一致性损失是Zero-DCE模型中用到的一种损失，因此它不是一种监督学习中的损失函数，而是一种Zero-Shot中的损失函数。它在低光增强和图像复原任务中很常见，目的是约束增强后的图像保持合理的色彩平衡，避免增强结果出现明显的 偏色（例如全图偏红、偏绿）。该损失函数的原理为：在“<strong>灰世界假设 (Gray-World Assumption)</strong>”中，自然图像的 R/G/B 通道平均值应当接近一致。损失函数通过惩罚通道均值之间的差异，使得增强后的图像颜色更自然。<br>计算方式:</p><ul><li>先计算 RGB 三个通道的均值：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="10.354ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 4576.5 658"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mi" transform="translate(636,-150) scale(0.707)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g></g><g data-mml-node="mo" transform="translate(1222.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1667.4,0)"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mi" transform="translate(636,-150) scale(0.707)"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g></g><g data-mml-node="mo" transform="translate(2909.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3353.8,0)"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mi" transform="translate(636,-150) scale(0.707)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g></g></g></g></svg></mjx-container></li><li>计算通道差异：</li></ul><p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="62.368ex" height="2.565ex" role="img" focusable="false" viewBox="0 -883.9 27566.7 1133.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="TeXAtom" transform="translate(861,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mi" transform="translate(759,0)"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2281.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(3337,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(3726,0)"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mi" transform="translate(636,-150) scale(0.707)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g></g><g data-mml-node="mo" transform="translate(5171,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(6171.2,0)"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mi" transform="translate(636,-150) scale(0.707)"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g></g><g data-mml-node="msup" transform="translate(7413,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mn" transform="translate(422,413) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(8238.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mstyle" transform="translate(8516.5,0)"><g data-mml-node="mspace"></g></g><g data-mml-node="msub" transform="translate(9683.2,0)"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="TeXAtom" transform="translate(861,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mi" transform="translate(759,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g></g></g><g data-mml-node="mo" transform="translate(11945.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(13001.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(13390.1,0)"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mi" transform="translate(636,-150) scale(0.707)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g></g><g data-mml-node="mo" transform="translate(14835,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(15835.3,0)"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mi" transform="translate(636,-150) scale(0.707)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g></g><g data-mml-node="msup" transform="translate(17058,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mn" transform="translate(422,413) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(17883.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mstyle" transform="translate(18161.5,0)"><g data-mml-node="mspace"></g></g><g data-mml-node="msub" transform="translate(19328.2,0)"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="TeXAtom" transform="translate(861,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g></g></g><g data-mml-node="mo" transform="translate(21609.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(22665.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(23054.2,0)"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mi" transform="translate(636,-150) scale(0.707)"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g></g><g data-mml-node="mo" transform="translate(24518.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(25518.4,0)"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mi" transform="translate(636,-150) scale(0.707)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g></g><g data-mml-node="msup" transform="translate(26741.1,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mn" transform="translate(422,413) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container></p><ul><li>计算颜色损失：</li></ul><p>$$<br>\mathcal{L}<em>{color} = \frac{1}{B} \sum</em>{b=1}^{B} \sqrt{ D_{RG}^{2} + D_{RB}^{2} + D_{GB}^{2}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">L_color</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(L_color, <span class="variable language_">self</span>).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x </span>):</span><br><span class="line">        b,c,h,w = x.shape</span><br><span class="line">        mean_rgb = torch.mean(x,[<span class="number">2</span>,<span class="number">3</span>],keepdim=<span class="literal">True</span>)</span><br><span class="line">        mr,mg, mb = torch.split(mean_rgb, <span class="number">1</span>, dim=<span class="number">1</span>)</span><br><span class="line">        Drg = torch.<span class="built_in">pow</span>(mr-mg,<span class="number">2</span>)</span><br><span class="line">        Drb = torch.<span class="built_in">pow</span>(mr-mb,<span class="number">2</span>)</span><br><span class="line">        Dgb = torch.<span class="built_in">pow</span>(mb-mg,<span class="number">2</span>)</span><br><span class="line">        k = torch.<span class="built_in">pow</span>(torch.<span class="built_in">pow</span>(Drg,<span class="number">2</span>) + torch.<span class="built_in">pow</span>(Drb,<span class="number">2</span>) + torch.<span class="built_in">pow</span>(Dgb,<span class="number">2</span>),<span class="number">0.5</span>)</span><br><span class="line">        <span class="keyword">return</span> k.mean()</span><br></pre></td></tr></table></figure><p>In the end:代码来源主要为主流低光增强模型的开源代码中，由于数量较多且各个模型所使用的损失函数多有重复，因此这里不再明确标注引用出处，对这些作者表示由衷感谢</p>]]></content>
      
      
      
        <tags>
            
            <tag> 低光增强 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fasterrcnn总结</title>
      <link href="/2025/04/16/fasterrcnn%E6%80%BB%E7%BB%93/"/>
      <url>/2025/04/16/fasterrcnn%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1>FaterRCNN代码学习总结1</h1><h2>1.项目结构</h2>backbone为FaterRCNN主干网络部分的代码，包括其权重，此开源项目提供了VGG和resnet50两种，并提供了对应的权重文件，代码的作者认为backbone部分已经在coco数据集上训练完善了，因此在新的训练中可以直接调用coco数据集的权重文件作为权重初始化。data为自己创建的COCO数据格式的月球陨石坑数据集network_files为除backbone以外的结构部分，如roi_head、rpn_function和boxes、transform等对预测框和图像进行处理转换的脚本。save_weights为训练过程每一轮权重的存放目录train_utils是一个自定义的Python工具模块，主要包含训练过程中常用的辅助函数和工具类<p><img src="images/fasterrcnn.jpg" alt=""></p><h2>2.train文件</h2><h3>2.1模块导入</h3><figure class="highlight plaintext"><figcaption><span>[lang:Python] import</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import datetime</span><br><span class="line">import torch</span><br><span class="line">from tqdm import tqdm  # 进度条显示库</span><br><span class="line">import transforms  # 数据预处理变换</span><br><span class="line">from network_files import FasterRCNN, FastRCNNPredictor  # 模型结构</span><br><span class="line">from backbone import resnet50_fpn_backbone  # 骨干网络</span><br><span class="line">from my_dataset import VOCDataSet  # VOC数据集读取(未使用)</span><br><span class="line">from datasets import CocoDataset  # COCO格式数据集读取</span><br><span class="line">from train_utils import GroupedBatchSampler, create_aspect_ratio_groups  # 数据采样工具</span><br><span class="line">from train_utils import train_eval_utils as utils  # 训练评估工具</span><br></pre></td></tr></table></figure><h3>2.2模型创建</h3>主要函数为def create_model(num_classes, load_pretrain_weights=True): 在该函数中调用backbone文件夹中的resnet50_fpn作为backbone ，之后使用FasterRCNN创建完整的model，导入预训练权重，更改roi_head中的分类头以适应自定义类别数，最后返回model（固定套路）。<figure class="highlight plaintext"><figcaption><span>[lang:Python] create_model</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def create_model(num_classes, load_pretrain_weights=True):</span><br><span class="line"></span><br><span class="line">    backbone = resnet50_fpn_backbone(pretrain_path=&quot;/disk527/Commondisk/a804_qkf/vscodeproject/code/faster_rcnn/backbone/resnet50.pth&quot;,</span><br><span class="line">                                     norm_layer=torch.nn.BatchNorm2d,</span><br><span class="line">                                     trainable_layers=3)</span><br><span class="line">    # 训练自己数据集时不要修改这里的91，修改的是传入的num_classes参数</span><br><span class="line">    model = FasterRCNN(backbone=backbone, num_classes=91)</span><br><span class="line">    if load_pretrain_weights:</span><br><span class="line">        # 载入预训练模型权重</span><br><span class="line">        # https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth</span><br><span class="line">        weights_dict = torch.load(&quot;/disk527/Commondisk/a804_qkf/vscodeproject/code/faster_rcnn/backbone/fasterrcnn_resnet50_fpn_coco.pth&quot;, map_location=&#x27;cpu&#x27;)</span><br><span class="line">        missing_keys, unexpected_keys = model.load_state_dict(weights_dict, strict=False)</span><br><span class="line">        if len(missing_keys) != 0 or len(unexpected_keys) != 0:</span><br><span class="line">            print(&quot;missing_keys: &quot;, missing_keys)</span><br><span class="line">            print(&quot;unexpected_keys: &quot;, unexpected_keys)</span><br><span class="line">    # get number of input features for the classifier</span><br><span class="line">    in_features = model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line">    # replace the pre-trained head with a new one</span><br><span class="line">    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)</span><br><span class="line"></span><br><span class="line">    return model</span><br></pre></td></tr></table></figure><h3>2.3主函数流程</h3><p>1.device 设备设置<br>2.data_transform = 数据增强设置<br>3.文件初始化<br>4.train_datasets、val_datasets加载，通过dataset进行数据集加载<br>5.train_data_loader、val_data_loader进行批次划分加载，使用函数torch.utils.data.DataLoader<br>6.创建模型 model = create_model(num_classes=args.num_classes + 1)<br><a href="http://7.model.to">7.model.to</a>(device)<br>8.优化器设置，用到函数：torch.optim.选择需要的迭代器<br>9.学习率设置，用到函数：torch.optim.lr_scheduler.StepLR<br>10.恢复训练设置（接着上次训练结果继续训练，可有可无）</p><figure class="highlight plaintext"><figcaption><span>[lang:Python] train</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">if args.resume != &quot;&quot;:</span><br><span class="line">        checkpoint = torch.load(args.resume, map_location=&#x27;cpu&#x27;)</span><br><span class="line">        model.load_state_dict(checkpoint[&#x27;model&#x27;])</span><br><span class="line">        optimizer.load_state_dict(checkpoint[&#x27;optimizer&#x27;])</span><br><span class="line">        lr_scheduler.load_state_dict(checkpoint[&#x27;lr_scheduler&#x27;])</span><br><span class="line">        args.start_epoch = checkpoint[&#x27;epoch&#x27;] + 1</span><br><span class="line">        if args.amp and &quot;scaler&quot; in checkpoint:</span><br><span class="line">            scaler.load_state_dict(checkpoint[&quot;scaler&quot;])</span><br><span class="line">        print(&quot;从epoch&#123;&#125;继续训练...&quot;.format(args.start_epoch))</span><br></pre></td></tr></table></figure><p>11.训练循环，主要针对train_loss、learning_rate、val_map进行更新，在循环前提前创建空列表，在循环中通过append进行记录，使用optimizer.step()进行参数更新</p><p>for epoch in tqdm(range(args.start_epoch, args.epochs), desc=“总进度”):<br>训练一个epoch： 使用函数utils.train_one_epoch（）；<br>记录指标： .appeend() 主要针对train_loss、learning_rate、val_map进行记录；<br>参数更新： lr_scheduler.step()；<br>在验证集上评估：utils.evaluate（）；<br>写入结果： with open；<br>保存模型权重： 权重文件要用到torch.save函数，在调用权重文件时，对应的用torch.load函数，这两个函数相关联；<br>循环结束<br>循环结束后，对训练过程的map loss 进行绘制</p><figure class="highlight plaintext"><figcaption><span>[lang:Python] plot_loss</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># plot loss and lr curve</span><br><span class="line"></span><br><span class="line">if len(train_loss) != 0 and len(learning_rate) != 0:</span><br><span class="line">    from plot_curve import plot_loss_and_lr</span><br><span class="line">    plot_loss_and_lr(train_loss, learning_rate)</span><br><span class="line"></span><br><span class="line"># plot mAP curve</span><br><span class="line"></span><br><span class="line">if len(val_map) != 0:</span><br><span class="line">    from plot_curve import plot_map</span><br><span class="line">    plot_map(val_map)</span><br></pre></td></tr></table></figure><h3>2.4 train_one_epoch函数</h3>rain_one_epoch函数并不位于trian文件中，该作者将其放至在了下面的文件下，但在train文件中进行了调用。目录：faster_rcnn/train_utils/train_eval_utils.py。<figure class="highlight plaintext"><figcaption><span>[lang:Python] def</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def train_one_epoch(model, optimizer, data_loader, device, epoch,</span><br><span class="line">                    print_freq=50, warmup=False, scaler=None):</span><br></pre></td></tr></table></figure><p>执行一个完整的训练周期<br>参数:<br>model: 待训练的PyTorch模型<br>optimizer: 优化器(如Adam/SGD)<br>data_loader: 训练数据加载器<br>device: 计算设备(‘cuda’/‘cpu’)<br>epoch: 当前周期序号<br>print_freq: 日志打印间隔(默认50个batch)<br>warmup: 是否启用学习率预热(默认False)<br>scaler: 混合精度训练的梯度缩放器(默认None)<br>1.初始化设置：设置为训练模式、指标记录器、添加学习率记录、日志头信息<br>2.warmup预热策略，避免训练初期因随机初始化导致梯度不稳定，逐步提高学习率</p><figure class="highlight plaintext"><figcaption><span>[lang:Python] warm</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if epoch == 0 and warmup is True:</span><br><span class="line">    warmup_factor = 1.0 / 1000  # 初始学习率缩放因子(从0.001倍开始)</span><br><span class="line">    warmup_iters = min(1000, len(data_loader) - 1)  # 预热步数(最多1000步)</span><br><span class="line">    lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)</span><br></pre></td></tr></table></figure><p>3.循环训练</p><figure class="highlight plaintext"><figcaption><span>[lang:Python] for</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i, [images, targets] in enumerate(metric_logger.log_every(data_loader, print_freq, header)):</span><br></pre></td></tr></table></figure><pre><code>数据转移到指定设备，使用.to(device)前向传播：model(images,targets)损失求和:：losses_reduced = sum(loss for loss in loss_dict_reduced.values())更新滑动平均损失检查损失是否有效反向传播与参数更新：清空梯度optimizer.zero_grad()、反向传播losses.backward() 、参数更新optimizer.step()、学习率更新lr_scheduler.step()记录指标</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Tmux--让服务器在后台继续干苦力</title>
      <link href="/2025/04/10/tmux/"/>
      <url>/2025/04/10/tmux/</url>
      
        <content type="html"><![CDATA[<p>Tmux是陈师兄介绍的方法，Tmux是一个终端复用器（terminal multiplexer），属于常用的开发工具，学会了之后可以大大的提高工作效率。<br>作为服务器窗口操作，主要可以用到的命令如下：（PS：本文作为笔者自己的笔记，感谢：<a href="https://blog.csdn.net/CSSDCC/article/details/121231906">https://blog.csdn.net/CSSDCC/article/details/121231906</a>  文章<br><img src="images/loading.jpg" alt=""></p><ol><li>新建窗口 tmux new -s your-session-name</li><li>在tmux窗口中，按下ctrl+b d或者输入以下命令，就会将当前session与窗口分离，session转到后台执行  tmux detach</li><li>退出session tmux kill-session -t your-session-name 该命令会直接杀死窗口</li><li>切换窗口：<br>ctrl+b c: 创建一个新窗口（状态栏会显示多个窗口的信息）<br>ctrl+b p: 切换到上一个窗口（按照状态栏的顺序）<br>ctrl+b n: 切换到下一个窗口<br>ctrl+b w: 从列表中选择窗口（这个最好用）</li><li>使用快捷键ctrl+b [ ，就可以通过方向键上下移动使用PageUp和PageDown可以实现上下翻页</li><li>注意！！！不要乱用ctrl+c 容易导致窗口关闭</li><li>重新回到窗口tmux attach -t your-session-name</li><li>窗口重命名tmux rename-session -t old-session new-session</li><li>划分窗格：# 划分为上下两个窗格 tmux split-window<br># 划分左右两个窗格 tmux split-window -h<br>左右划分：ctrl+b %<br>上下划分：ctrl+b &quot;</li><li>其他操作<br>列出所有快捷键，及其对应的 Tmux 命令： $ tmux list-keys<br>列出所有 Tmux 命令及其参数： $ tmux list-commands<br>列出当前所有 Tmux 会话的信息 ：$ tmux info<br>重新加载当前的 Tmux 配置： $ tmux source-file ~/.tmux.conf</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>投影方式</title>
      <link href="/2025/04/08/%E6%8A%95%E5%BD%B1%E6%96%B9%E5%BC%8F/"/>
      <url>/2025/04/08/%E6%8A%95%E5%BD%B1%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>等距圆柱投影</p><p>等距圆柱投影方式如图所示：<br><img src="https://img2023.cnblogs.com/blog/981804/202306/981804-20230602142021107-871401001.png" alt="图片" title="等距圆柱投影示意图"><br><img src="https://img2023.cnblogs.com/blog/981804/202306/981804-20230602143153357-1081762523.png" alt="图片" title="等距圆柱投影示意图2"><br>等距圆柱投影（Equidistant Cylindrical Projection）是一种简单的地图投影方法，属于圆柱投影的一种。它的特点是保持经线和纬线均为等距的平行直线，形成规则的网格<br>基本特性<br>经纬线形状：<br>经线：等间隔的平行竖直线。</p><p>纬线：等间隔的平行水平线，与经线垂直。</p><p>网格呈矩形，类似棋盘。</p><p>等距性质：</p><p>沿经线方向：任意两条纬线之间的距离与实际地球表面相同（纬度间隔保持等距）。</p><p>沿纬线方向：赤道上的比例尺是真实的，但高纬度地区的东西方向会被拉伸（因纬线长度与实际不符）。</p><p>变形：</p><p>角度变形：所有纬线和经线的交点处角度保持不变（局部形状在小范围内近似），但整体地图存在显著的角度变形，尤其是高纬度地区。</p><p>面积变形：高纬度地区的面积会被夸大（例如极地地区显得异常大）。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 投影 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
